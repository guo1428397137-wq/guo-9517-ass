{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1tXFfiz8fAMxMke_83cpNc3Cc1bxH_bMC",
      "authorship_tag": "ABX9TyMJQK8yeZ1EN63xAiT2EDt0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guo1428397137-wq/guo-9517-ass/blob/Faster-R-CNN/%E2%80%9CUntitled2_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.ops import nms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import json\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# Basic config\n",
        "DATA_ROOT = Path(\"/content/drive/MyDrive/archive\")\n",
        "\n",
        "TRAIN_IMG_DIR   = DATA_ROOT / \"train\" / \"images\"\n",
        "TRAIN_LABEL_DIR = DATA_ROOT / \"train\" / \"labels\"\n",
        "VAL_IMG_DIR     = DATA_ROOT / \"valid\" / \"images\"\n",
        "VAL_LABEL_DIR   = DATA_ROOT / \"valid\" / \"labels\"\n",
        "\n",
        "NUM_CLASSES   = 12\n",
        "BATCH_SIZE    = 12  # L4可以支持更大的batch\n",
        "IMG_MAX_SIZE  = 640  # 保持640\n",
        "EPOCHS        = 15  # 15轮应该足够\n",
        "LR            = 0.01  # 高学习率配合大batch\n",
        "WEIGHT_DECAY  = 5e-4\n",
        "SCORE_THRESH  = 0.05\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(\"/content/\", exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    'background',\n",
        "    'Ants', 'Bees', 'Beetles', 'Caterpillars',\n",
        "    'Earthworms', 'Earwigs', 'Grasshoppers', 'Moths',\n",
        "    'Slugs', 'Snails', 'Wasps', 'Weevils'\n",
        "]\n",
        "\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LR}\")\n",
        "print(f\"  Number of training rounds: {EPOCHS}\")\n",
        "print(f\"  Score threshold: {SCORE_THRESH}\")\n",
        "\n",
        "\n",
        "# Read labels\n",
        "def load_yolo_labels(label_path: Path, img_w: int, img_h: int):\n",
        "\n",
        "    if not label_path.exists():\n",
        "        return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "    try:\n",
        "        data = np.loadtxt(str(label_path), ndmin=2)\n",
        "    except Exception:\n",
        "        return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "    if data.shape[0] == 0:\n",
        "        return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "    cls = data[:, 0].astype(np.int64)\n",
        "    cx  = data[:, 1] * img_w\n",
        "    cy  = data[:, 2] * img_h\n",
        "    bw  = data[:, 3] * img_w\n",
        "    bh  = data[:, 4] * img_h\n",
        "\n",
        "    x1 = np.clip(cx - bw / 2, 0, img_w)\n",
        "    y1 = np.clip(cy - bh / 2, 0, img_h)\n",
        "    x2 = np.clip(cx + bw / 2, 0, img_w)\n",
        "    y2 = np.clip(cy + bh / 2, 0, img_h)\n",
        "\n",
        "    valid = (x2 > x1 + 5) & (y2 > y1 + 5)\n",
        "    if not valid.any():\n",
        "        return torch.zeros((0, 4), dtype=torch.float32), torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "    boxes = np.stack([x1[valid], y1[valid], x2[valid], y2[valid]], axis=1).astype(np.float32)\n",
        "    labels = cls[valid] + 1\n",
        "\n",
        "    return torch.as_tensor(boxes, dtype=torch.float32), torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "def resize_keep_ratio(img: np.ndarray, target: Dict[str, Any], max_size: int):\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    scale = min(max_size / max(h, w), 1.0)\n",
        "    nh, nw = int(h * scale), int(w * scale)\n",
        "\n",
        "    if (nh, nw) != (h, w):\n",
        "        img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_LINEAR)\n",
        "        if target[\"boxes\"].numel() > 0:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes[:, [0, 2]] *= float(nw) / w\n",
        "            boxes[:, [1, 3]] *= float(nh) / h\n",
        "            boxes[:, 0] = torch.clamp(boxes[:, 0], 0, nw)\n",
        "            boxes[:, 1] = torch.clamp(boxes[:, 1], 0, nh)\n",
        "            boxes[:, 2] = torch.clamp(boxes[:, 2], 0, nw)\n",
        "            boxes[:, 3] = torch.clamp(boxes[:, 3], 0, nh)\n",
        "            target[\"boxes\"] = boxes\n",
        "\n",
        "    target[\"size\"] = torch.tensor([nh, nw], dtype=torch.int64)\n",
        "    return img, target\n",
        "\n",
        "# Dataset\n",
        "class YoloDetectionDataset(Dataset):\n",
        "    def __init__(self, img_dir: Path, label_dir: Path, training: bool = True, img_max_size: int = 800):\n",
        "        self.img_dir   = Path(img_dir)\n",
        "        self.label_dir = Path(label_dir)\n",
        "        self.training  = training\n",
        "        self.img_max_size = img_max_size\n",
        "\n",
        "        exts = [\".jpg\", \".jpeg\", \".png\"]\n",
        "        self.img_paths = [p for p in self.img_dir.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "        self.img_paths.sort()\n",
        "\n",
        "        if len(self.img_paths) == 0:\n",
        "            raise RuntimeError(f\"No images found in {self.img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        img_path = self.img_paths[idx]\n",
        "        label_path = self.label_dir / (img_path.stem + \".txt\")\n",
        "\n",
        "        img = cv2.imread(str(img_path))\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        boxes, labels = load_yolo_labels(label_path, w, h)\n",
        "\n",
        "        if boxes.numel() > 0:\n",
        "            area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "        else:\n",
        "            area = torch.zeros((0,), dtype=torch.float32)\n",
        "\n",
        "        iscrowd = torch.zeros((labels.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\":    boxes,\n",
        "            \"labels\":   labels,\n",
        "            \"image_id\": torch.tensor([idx]),\n",
        "            \"area\":     area,\n",
        "            \"iscrowd\":  iscrowd,\n",
        "        }\n",
        "\n",
        "        if self.training and boxes.numel() > 0:\n",
        "\n",
        "            if np.random.rand() < 0.5:\n",
        "                img = np.ascontiguousarray(img[:, ::-1])\n",
        "                w_img = img.shape[1]\n",
        "                b = target[\"boxes\"].clone()\n",
        "                x1 = b[:, 0].clone()\n",
        "                x2 = b[:, 2].clone()\n",
        "                b[:, 0] = w_img - x2\n",
        "                b[:, 2] = w_img - x1\n",
        "                target[\"boxes\"] = b\n",
        "\n",
        "            if np.random.rand() < 0.3:\n",
        "                img = np.ascontiguousarray(img[::-1, :])\n",
        "                h_img = img.shape[0]\n",
        "                b = target[\"boxes\"].clone()\n",
        "                y1 = b[:, 1].clone()\n",
        "                y2 = b[:, 3].clone()\n",
        "                b[:, 1] = h_img - y2\n",
        "                b[:, 3] = h_img - y1\n",
        "                target[\"boxes\"] = b\n",
        "\n",
        "            if np.random.rand() < 0.5:\n",
        "                img = cv2.convertScaleAbs(\n",
        "                    img,\n",
        "                    alpha=np.random.uniform(0.7, 1.3),\n",
        "                    beta=np.random.uniform(-20, 20),\n",
        "                )\n",
        "\n",
        "        img, target = resize_keep_ratio(img, target, self.img_max_size)\n",
        "\n",
        "        img = TF.to_tensor(img)\n",
        "        img = TF.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs, targets = list(zip(*batch))\n",
        "    return list(imgs), list(targets)\n",
        "\n",
        "#  DataLoaders\n",
        "train_dataset = YoloDetectionDataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, training=True,  img_max_size=IMG_MAX_SIZE)\n",
        "val_dataset   = YoloDetectionDataset(VAL_IMG_DIR,   VAL_LABEL_DIR,   training=False, img_max_size=IMG_MAX_SIZE)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=2, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=2, collate_fn=collate_fn, pin_memory=True)\n",
        "\n",
        "print(f\"\\nTrain samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "\n",
        "# check sample\n",
        "img, target = train_dataset[0]\n",
        "print(\"\\nFirst sample:\")\n",
        "print(\"Image shape:\", img.shape)\n",
        "print(\"Boxes shape:\", target[\"boxes\"].shape)\n",
        "print(\"Labels:\", target[\"labels\"])\n",
        "\n",
        "# Model\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES + 1)\n",
        "\n",
        "model.to(device)\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer    = torch.optim.SGD(params, lr=LR, momentum=0.9, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # 第7轮降低学习率\n",
        "scaler       = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n",
        "\n",
        "# Train & eval\n",
        "def train_one_epoch(epoch: int):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d}\")\n",
        "    running_loss = 0.0\n",
        "    loss_components = {'cls': 0, 'box': 0, 'obj': 0, 'rpn': 0}\n",
        "\n",
        "    for images, targets in pbar:\n",
        "        images  = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
        "            loss_dict = model(images, targets)\n",
        "            loss = sum(loss_dict.values())\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        for key in loss_components:\n",
        "            loss_key = {\n",
        "                'cls': 'loss_classifier',\n",
        "                'box': 'loss_box_reg',\n",
        "                'obj': 'loss_objectness',\n",
        "                'rpn': 'loss_rpn_box_reg'\n",
        "            }[key]\n",
        "            loss_components[key] += loss_dict.get(loss_key, torch.tensor(0)).item()\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    n = len(train_loader)\n",
        "    return running_loss / n, {k: v/n for k, v in loss_components.items()}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_map():\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
        "    pbar = tqdm(val_loader, desc=\"Eval\")\n",
        "\n",
        "    for images, targets in pbar:\n",
        "        images  = [img.to(device) for img in images]\n",
        "        outputs = model(images)\n",
        "\n",
        "        preds, gts = [], []\n",
        "        for out, tgt in zip(outputs, targets):\n",
        "            preds.append({\n",
        "                \"boxes\":  out[\"boxes\"].detach().cpu(),\n",
        "                \"scores\": out[\"scores\"].detach().cpu(),\n",
        "                \"labels\": out[\"labels\"].detach().cpu(),\n",
        "            })\n",
        "            gts.append({\n",
        "                \"boxes\":   tgt[\"boxes\"].detach().cpu(),\n",
        "                \"labels\":  tgt[\"labels\"].detach().cpu(),\n",
        "                \"iscrowd\": tgt.get(\"iscrowd\", torch.zeros(tgt[\"labels\"].shape[0], dtype=torch.int64)).detach().cpu()\n",
        "            })\n",
        "        metric.update(preds, gts)\n",
        "\n",
        "    res = metric.compute()\n",
        "    return {\n",
        "        \"map\": res.get(\"map\", torch.tensor(0)).item(),\n",
        "        \"map_50\": res.get(\"map_50\", torch.tensor(0)).item(),\n",
        "        \"map_75\": res.get(\"map_75\", torch.tensor(0)).item(),\n",
        "    }\n",
        "\n",
        "# Training loop\n",
        "best_map = 0.0\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'map': [],\n",
        "    'map_50': [],\n",
        "    'map_75': [],\n",
        "    'loss_cls': [],\n",
        "    'loss_box': [],\n",
        "    'lr': []\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Start training\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, loss_comp = train_one_epoch(epoch)\n",
        "    metrics = evaluate_map()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['map'].append(metrics['map'])\n",
        "    history['map_50'].append(metrics['map_50'])\n",
        "    history['map_75'].append(metrics.get('map_75', 0))\n",
        "    history['loss_cls'].append(loss_comp['cls'])\n",
        "    history['loss_box'].append(loss_comp['box'])\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    print(f\"[Epoch {epoch:02d}/{EPOCHS}] \"\n",
        "          f\"Loss={train_loss:.4f} | \"\n",
        "          f\"mAP={metrics['map']:.4f} | \"\n",
        "          f\"mAP@50={metrics['map_50']:.4f} | \"\n",
        "          f\"LR={current_lr:.6f}\")\n",
        "\n",
        "    if metrics['map'] > best_map:\n",
        "        best_map = metrics['map']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'map': metrics['map'],\n",
        "            'history': history\n",
        "        }, \"/content/fasterrcnn_best.pth\")\n",
        "        print(\"Saved best model\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nTraining finished in {training_time/60:.2f} minutes\")\n",
        "print(f\"Best mAP: {best_map:.4f}\")\n",
        "\n",
        "\n",
        "with open('/content/training_history.json', 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Plot\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Loss\n",
        "axes[0, 0].plot(history['train_loss'], marker='o', label='Total Loss', linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
        "axes[0, 0].set_title('Training Loss', fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# mAP\n",
        "axes[0, 1].plot(history['map'], marker='o', label='mAP', linewidth=2)\n",
        "axes[0, 1].plot(history['map_50'], marker='s', label='mAP@50', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 1].set_ylabel('mAP', fontsize=11)\n",
        "axes[0, 1].set_title('Validation mAP', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss components\n",
        "axes[1, 0].plot(history['loss_cls'], marker='o', label='Classification', linewidth=2)\n",
        "axes[1, 0].plot(history['loss_box'], marker='s', label='Box Regression', linewidth=2)\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Loss', fontsize=11)\n",
        "axes[1, 0].set_title('Loss Components', fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate\n",
        "axes[1, 1].plot(history['lr'], marker='o', color='orangered', linewidth=2)\n",
        "axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Learning Rate', fontsize=11)\n",
        "axes[1, 1].set_title('Learning Rate Schedule', fontsize=13, fontweight='bold')\n",
        "axes[1, 1].set_yscale('log')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Visualization\n",
        "def denormalize_image(img_tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img = img_tensor * std + mean\n",
        "    return torch.clamp(img, 0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "\n",
        "def visualize_predictions(dataset, num_images=6, score_thresh=SCORE_THRESH, nms_thresh=0.5):\n",
        "\n",
        "    model.eval()\n",
        "    idxs = random.sample(range(len(dataset)), k=min(num_images, len(dataset)))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, idx in enumerate(idxs):\n",
        "        img, target = dataset[idx]\n",
        "        img_np = denormalize_image(img.cpu())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model([img.to(device)])[0]\n",
        "\n",
        "        boxes  = out[\"boxes\"].cpu()\n",
        "        scores = out[\"scores\"].cpu()\n",
        "        labels = out[\"labels\"].cpu()\n",
        "\n",
        "        keep_conf = scores >= score_thresh\n",
        "        boxes = boxes[keep_conf]\n",
        "        scores = scores[keep_conf]\n",
        "        labels = labels[keep_conf]\n",
        "\n",
        "        if len(boxes) > 0:\n",
        "            final_boxes, final_scores, final_labels = [], [], []\n",
        "            for cls_id in torch.unique(labels):\n",
        "                cls_mask = labels == cls_id\n",
        "                cls_boxes = boxes[cls_mask]\n",
        "                cls_scores = scores[cls_mask]\n",
        "\n",
        "                keep_nms = nms(cls_boxes, cls_scores, nms_thresh)\n",
        "\n",
        "                final_boxes.append(cls_boxes[keep_nms])\n",
        "                final_scores.append(cls_scores[keep_nms])\n",
        "                final_labels.append(labels[cls_mask][keep_nms])\n",
        "\n",
        "            boxes = torch.cat(final_boxes).numpy()\n",
        "            scores = torch.cat(final_scores).numpy()\n",
        "            labels = torch.cat(final_labels).numpy()\n",
        "        else:\n",
        "            boxes = boxes.numpy()\n",
        "            scores = scores.numpy()\n",
        "            labels = labels.numpy()\n",
        "\n",
        "        vis = (img_np * 255).astype(np.uint8).copy()\n",
        "\n",
        "        for (x1, y1, x2, y2), s, lb in zip(boxes, scores, labels):\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "            cv2.rectangle(vis, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "            class_name = CLASS_NAMES[int(lb)] if int(lb) < len(CLASS_NAMES) else str(int(lb))\n",
        "            text = f\"{class_name}: {s:.2f}\"\n",
        "            (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "            cv2.rectangle(vis, (x1, max(0, y1-th-5)), (x1+tw, y1), (255, 0, 0), -1)\n",
        "            cv2.putText(vis, text, (x1, max(th, y1-5)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
        "\n",
        "        gt_boxes = target[\"boxes\"].numpy()\n",
        "        for (x1, y1, x2, y2) in gt_boxes:\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "            cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        axes[i].imshow(vis)\n",
        "        axes[i].axis('off')\n",
        "        axes[i].set_title(f'Sample {idx} | {len(boxes)} pred boxes', fontsize=10, fontweight='bold')\n",
        "\n",
        "    from matplotlib.patches import Patch\n",
        "    legend = [\n",
        "        Patch(facecolor='red', label='Prediction'),\n",
        "        Patch(facecolor='green', label='Ground Truth')\n",
        "    ]\n",
        "    fig.legend(handles=legend, loc='upper right', fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/predictions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nVisualizing predictions (with NMS):\")\n",
        "visualize_predictions(val_dataset, num_images=6, score_thresh=SCORE_THRESH, nms_thresh=0.5)\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def comprehensive_evaluation(dataset, score_thresh=SCORE_THRESH, iou_thresh=0.5):\n",
        "    \"\"\"\n",
        "    全面评估：混淆矩阵、Precision/Recall/F1/Accuracy、AUC\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    all_gt_labels = []\n",
        "    all_pred_labels = []\n",
        "    all_pred_scores = []\n",
        "    all_correct = []\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Running comprehensive evaluation...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for idx in tqdm(range(len(dataset)), desc='Evaluating'):\n",
        "        img, target = dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model([img.to(device)])[0]\n",
        "\n",
        "        pred_boxes = out[\"boxes\"].cpu().numpy()\n",
        "        pred_scores = out[\"scores\"].cpu().numpy()\n",
        "        pred_labels = out[\"labels\"].cpu().numpy()\n",
        "\n",
        "        gt_boxes = target[\"boxes\"].numpy()\n",
        "        gt_labels = target[\"labels\"].numpy()\n",
        "\n",
        "\n",
        "        keep = pred_scores >= score_thresh\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_scores = pred_scores[keep]\n",
        "        pred_labels = pred_labels[keep]\n",
        "\n",
        "\n",
        "        matched_gt = set()\n",
        "        for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "            best_iou = 0\n",
        "            best_gt_idx = -1\n",
        "\n",
        "            for gt_idx, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):\n",
        "                if gt_idx in matched_gt:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                x1 = max(pred_box[0], gt_box[0])\n",
        "                y1 = max(pred_box[1], gt_box[1])\n",
        "                x2 = min(pred_box[2], gt_box[2])\n",
        "                y2 = min(pred_box[3], gt_box[3])\n",
        "\n",
        "                if x2 > x1 and y2 > y1:\n",
        "                    intersection = (x2 - x1) * (y2 - y1)\n",
        "                    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "                    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
        "                    union = pred_area + gt_area - intersection\n",
        "                    iou = intersection / union if union > 0 else 0\n",
        "\n",
        "                    if iou > best_iou:\n",
        "                        best_iou = iou\n",
        "                        best_gt_idx = gt_idx\n",
        "\n",
        "\n",
        "            if best_iou >= iou_thresh and best_gt_idx >= 0:\n",
        "                gt_label = gt_labels[best_gt_idx]\n",
        "                if pred_label == gt_label:\n",
        "                    all_correct.append(1)\n",
        "                    matched_gt.add(best_gt_idx)\n",
        "                    y_true.append(gt_label)\n",
        "                    y_pred.append(pred_label)\n",
        "                else:\n",
        "                    all_correct.append(0)\n",
        "                    y_true.append(gt_label)\n",
        "                    y_pred.append(pred_label)\n",
        "            else:\n",
        "                all_correct.append(0)\n",
        "\n",
        "                y_pred.append(pred_label)\n",
        "\n",
        "                y_true.append(0)\n",
        "\n",
        "            all_pred_scores.append(pred_score)\n",
        "\n",
        "\n",
        "        for gt_idx, gt_label in enumerate(gt_labels):\n",
        "            if gt_idx not in matched_gt:\n",
        "                all_correct.append(0)\n",
        "                y_true.append(gt_label)\n",
        "                y_pred.append(0)\n",
        "                all_pred_scores.append(0.0)\n",
        "\n",
        "\n",
        "    all_correct = np.array(all_correct)\n",
        "    all_pred_scores = np.array(all_pred_scores)\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    print(f\"\\nTotal predictions: {len(all_correct)}\")\n",
        "    print(f\"Correct predictions: {all_correct.sum()}\")\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(NUM_CLASSES + 1)))\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES,\n",
        "                yticklabels=CLASS_NAMES,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"\\nPer-class metrics:\")\n",
        "\n",
        "    class_metrics = {}\n",
        "    for label in range(1, NUM_CLASSES + 1):\n",
        "        mask_true = (y_true == label)\n",
        "        mask_pred = (y_pred == label)\n",
        "\n",
        "        tp = np.sum(mask_true & mask_pred)\n",
        "        fp = np.sum(~mask_true & mask_pred)\n",
        "        fn = np.sum(mask_true & ~mask_pred)\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        support = np.sum(mask_true)\n",
        "\n",
        "        class_metrics[label] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'support': support\n",
        "        }\n",
        "\n",
        "        class_name = CLASS_NAMES[label].lower()\n",
        "\n",
        "        idx = f\"{label-1:02d}\"\n",
        "        print(f\"{idx} {class_name:<13} | P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f} AUC: -- S: {support:>3d}\")\n",
        "\n",
        "\n",
        "    print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nFull classification report:\")\n",
        "    print(f\"{'':>15} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "    for label in range(1, NUM_CLASSES + 1):\n",
        "        class_name = CLASS_NAMES[label].lower()\n",
        "        m = class_metrics[label]\n",
        "        print(f\"{class_name:>15} {m['precision']:10.4f} {m['recall']:10.4f} {m['f1']:10.4f} {m['support']:10d}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    mask_objects = (y_true > 0) | (y_pred > 0)\n",
        "    y_true_obj = y_true[mask_objects]\n",
        "    y_pred_obj = y_pred[mask_objects]\n",
        "\n",
        "\n",
        "    overall_accuracy = accuracy_score(y_true_obj, y_pred_obj)\n",
        "    macro_precision = precision_score(y_true_obj, y_pred_obj, average='macro', zero_division=0)\n",
        "    macro_recall = recall_score(y_true_obj, y_pred_obj, average='macro', zero_division=0)\n",
        "    macro_f1 = f1_score(y_true_obj, y_pred_obj, average='macro', zero_division=0)\n",
        "\n",
        "    overall_precision = precision_score(y_true_obj, y_pred_obj, average='weighted', zero_division=0)\n",
        "    overall_recall = recall_score(y_true_obj, y_pred_obj, average='weighted', zero_division=0)\n",
        "    overall_f1 = f1_score(y_true_obj, y_pred_obj, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"{'accuracy':>15} {overall_accuracy:34.4f} {len(y_true_obj):10d}\")\n",
        "    print(f\"{'macro avg':>15} {macro_precision:10.4f} {macro_recall:10.4f} {macro_f1:10.4f} {len(y_true_obj):10d}\")\n",
        "    print(f\"{'weighted avg':>15} {overall_precision:10.4f} {overall_recall:10.4f} {overall_f1:10.4f} {len(y_true_obj):10d}\")\n",
        "\n",
        "\n",
        "    if len(all_correct) > 0 and len(np.unique(all_correct)) > 1:\n",
        "        # AUC\n",
        "        auc_score = roc_auc_score(all_correct, all_pred_scores)\n",
        "        print(f\"\\nAUC (Correct vs Incorrect): {auc_score:.4f}\")\n",
        "    else:\n",
        "        auc_score = None\n",
        "\n",
        " =\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=CLASS_NAMES,\n",
        "                yticklabels=CLASS_NAMES,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    evaluation_results = {\n",
        "        'overall': {\n",
        "            'precision_weighted': float(overall_precision),\n",
        "            'recall_weighted': float(overall_recall),\n",
        "            'f1_weighted': float(overall_f1),\n",
        "            'accuracy': float(overall_accuracy),\n",
        "            'precision_macro': float(macro_precision),\n",
        "            'recall_macro': float(macro_recall),\n",
        "            'f1_macro': float(macro_f1),\n",
        "            'auc': float(auc_score) if auc_score is not None else None\n",
        "        },\n",
        "        'per_class': {\n",
        "            CLASS_NAMES[i]: {\n",
        "                'precision': float(class_metrics[i]['precision']),\n",
        "                'recall': float(class_metrics[i]['recall']),\n",
        "                'f1': float(class_metrics[i]['f1']),\n",
        "                'support': int(class_metrics[i]['support'])\n",
        "            } for i in range(1, NUM_CLASSES + 1)\n",
        "        },\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "        'settings': {\n",
        "            'score_threshold': score_thresh,\n",
        "            'iou_threshold': iou_thresh\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('/content/evaluation_results.json', 'w') as f:\n",
        "        json.dump(evaluation_results, f, indent=2)\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "\n",
        "\n",
        "eval_results = comprehensive_evaluation(val_dataset, score_thresh=SCORE_THRESH, iou_thresh=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSd5hG3TpJqW",
        "outputId": "4f49ef46-0ff3-4b0c-fff8-674282c4a694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Using device: cuda\n",
            "  Batch Size: 12\n",
            "  Learning rate: 0.01\n",
            "  Number of training rounds: 15\n",
            "  Score threshold: 0.05\n",
            "\n",
            "Train samples: 11502, Val samples: 1095\n",
            "\n",
            "First sample:\n",
            "Image shape: torch.Size([3, 640, 640])\n",
            "Boxes shape: torch.Size([1, 4])\n",
            "Labels: tensor([11])\n",
            "\n",
            "Model parameters: 41,355,536\n",
            "\n",
            "======================================================================\n",
            "Start training\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 01: 100%|██████████| 959/959 [1:09:02<00:00,  4.32s/it, loss=1.342]\n",
            "Eval: 100%|██████████| 92/92 [12:40<00:00,  8.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01/15] Loss=2.2997 | mAP=0.0000 | mAP@50=0.0000 | LR=0.010000\n",
            "Saved best model\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 02: 100%|██████████| 959/959 [13:45<00:00,  1.16it/s, loss=0.739]\n",
            "Eval: 100%|██████████| 92/92 [00:47<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 02/15] Loss=1.0981 | mAP=0.0001 | mAP@50=0.0004 | LR=0.010000\n",
            "Saved best model\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 03: 100%|██████████| 959/959 [13:46<00:00,  1.16it/s, loss=0.739]\n",
            "Eval: 100%|██████████| 92/92 [00:47<00:00,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 03/15] Loss=0.7411 | mAP=0.0001 | mAP@50=0.0008 | LR=0.010000\n",
            "Saved best model\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 04: 100%|██████████| 959/959 [13:48<00:00,  1.16it/s, loss=1.036]\n",
            "Eval: 100%|██████████| 92/92 [00:48<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 04/15] Loss=0.7523 | mAP=0.0001 | mAP@50=0.0006 | LR=0.010000\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 05: 100%|██████████| 959/959 [13:50<00:00,  1.15it/s, loss=1.065]\n",
            "Eval: 100%|██████████| 92/92 [00:49<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 05/15] Loss=0.8738 | mAP=0.0001 | mAP@50=0.0006 | LR=0.010000\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06: 100%|██████████| 959/959 [13:52<00:00,  1.15it/s, loss=1.187]\n",
            "Eval: 100%|██████████| 92/92 [00:50<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 06/15] Loss=1.0915 | mAP=0.0002 | mAP@50=0.0009 | LR=0.010000\n",
            "Saved best model\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 07: 100%|██████████| 959/959 [13:54<00:00,  1.15it/s, loss=1.455]\n",
            "Eval: 100%|██████████| 92/92 [00:50<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 07/15] Loss=1.2831 | mAP=0.0002 | mAP@50=0.0009 | LR=0.001000\n",
            "Saved best model\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 08:   4%|▍         | 36/959 [00:32<13:23,  1.15it/s, loss=1.452]"
          ]
        }
      ]
    }
  ]
}